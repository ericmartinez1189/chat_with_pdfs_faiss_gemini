{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Notebook for Processing PDFs and Answering Questions with Gemini\n",
        "\n",
        "**Description:**\n",
        "\n",
        "This Colab notebook demonstrates a workflow for processing a collection of PDFs, extracting text, create vectors using FAISS and usin Gemini  to answer questions about the content."
      ],
      "metadata": {
        "id": "clMebgVScPFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set up the enviroment**"
      ],
      "metadata": {
        "id": "XcPGiVd3YTBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6MI3cLzYOKN"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai\n",
        "%pip install langchain-community\n",
        "%pip install pypdf\n",
        "%pip install langchain\n",
        "%pip install -U langchain-community faiss-cpu langchain-openai tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set up google credentials**"
      ],
      "metadata": {
        "id": "-Glkqef9YZKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Access your Gemini API key\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_secret_name = 'GoogleAIStudio'  # @param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "  GOOGLE_API_KEY=userdata.get(gemini_api_secret_name)\n",
        "  genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except userdata.SecretNotFoundError as e:\n",
        "   print(f'''Secret not found\\n\\nThis expects you to create a secret named {gemini_api_secret_name} in Colab\\n\\nVisit https://makersuite.google.com/app/apikey to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {gemini_api_secret_name}''')\n",
        "   raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "  print(f'''You need to grant this notebook access to the {gemini_api_secret_name} secret in order for the notebook to access Gemini on your behalf.''')\n",
        "  raise e\n",
        "except Exception as e:\n",
        "  # unknown error\n",
        "  print(f\"There was an unknown error. Ensure you have a secret {gemini_api_secret_name} stored in Colab and it's a valid key from https://makersuite.google.com/app/apikey\")\n",
        "  raise e\n"
      ],
      "metadata": {
        "id": "1Iebm8A-Z06-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Get all text found as pdfs into a drive folder**\n",
        "\n",
        "Create a folder and copy and paste the path below:"
      ],
      "metadata": {
        "id": "LPLrH5NnYmpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder path containing the PDFs\n",
        "folder_path = \"/content/drive/MyDrive/books_pdfs\" # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "tAualDl-ZLzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Function to extract text from individual PDF\n",
        "def get_pdf_text(filepath):\n",
        "  loader = PyPDFLoader(filepath)\n",
        "  pdf_pages = loader.load_and_split()\n",
        "  text = \"\"\n",
        "  for page in pdf_pages:\n",
        "    text += page.page_content + \"\\n\"\n",
        "  return text\n",
        "\n",
        "# Function to process all PDFs in the folder\n",
        "def process_all_pdfs():\n",
        "  all_text = \"\"\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "      filepath = os.path.join(folder_path, filename)\n",
        "      text = get_pdf_text(filepath)\n",
        "      all_text += f\"--- Text from {filename} ---\\n{text}\\n\"\n",
        "  return all_text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Call the processing function\n",
        "all_text = process_all_pdfs()\n",
        "chunks = get_text_chunks(all_text)\n"
      ],
      "metadata": {
        "id": "BWbTK2XLnJaT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create a FAISS vector store locally**"
      ],
      "metadata": {
        "id": "m5PCL1_Ia6S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "    db = FAISS.from_texts(chunks,embeddings)\n",
        "    db.save_local(\"faiss_index\")\n",
        "\n",
        "get_vector_store(chunks)"
      ],
      "metadata": {
        "id": "Gy7Yw6aE6sTY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create a conversation chain with gemini**"
      ],
      "metadata": {
        "id": "4hfDfiCRbUMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_conversational_chain():\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context,\n",
        "    making sure to provide all the details. If the answer is not available\n",
        "    in the context, just say \"answer is not available in the context,\"\n",
        "    and do not provide the wrong answer.\n",
        "\n",
        "    Context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.3, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "    return chain\n",
        "\n"
      ],
      "metadata": {
        "id": "Os-46s5o4krI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "user_input = \"What is PEFT?\" # @param {type: \"string\"}\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "docs = new_db.similarity_search(user_input)\n",
        "\n",
        "# Prepare the input dictionary with context and question\n",
        "input_data = {\"input_documents\": docs , \"question\": user_input}\n",
        "\n",
        "chain = get_conversational_chain()\n",
        "\n",
        "# Process the input with the chain and retrieve the response\n",
        "response = chain(input_data, return_only_outputs=True)\n",
        "\n",
        "\n",
        "print(response['output_text'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yuim_unp_HMO",
        "outputId": "7e3ccc61-8737-447e-b6d1-b2bed11fbea0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Parameter-efficient fine-tuning (PEFT) is a set of techniques that allow you to fine-tune your models while utilizing less compute resources. It involves freezing the parameters of the pre-trained model and fine-tuning a smaller set of parameters.\n"
          ]
        }
      ]
    }
  ]
}